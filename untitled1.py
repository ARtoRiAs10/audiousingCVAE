# -*- coding: utf-8 -*-
"""Untitled1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1DjpixraCUjOzXhScr7OHNyJuwspXIuq4
"""



import pandas as pd
import numpy as np
import tensorflow as tf
import matplotlib.pyplot as plt
import librosa

import os

import csv

import tensorflow as tf
import tensorflow_probability as tpa
from tensorflow import keras
from tensorflow.keras import layers

from IPython import display
from IPython.display import clear_output

import glob
import imageio
import time
import IPython.display as ipd

AUTOTUNE = tf.data.experimental.AUTOTUNE







from tensorflow.python.client import session
seed=123
tf.compat.v1.set_random_seed(seed)
session_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)
sess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(), config=session_conf)
tf.compat.v1.keras.backend.set_session(sess)

train_size = 60000
BATCH_SIZE= 10
test_size=10000
epochs=20

latent_dim=2
num_examples_to_generate = 10

BASE_PATH = '../input/gtzan-dataset-music-genre-classification/Data/genres_original'



"""Data preprocessing

"""

def DatasetLoader(class_):
  music_list = np.array(sorted(os.listdir(BASE_PATH+'/'+class_)))
  train_music_1= list(music_list[[0,52,19,39,71,12,75,85,3,45,24,46,88]])

  train_music_2= list(music_list[[4,43,56,55,45,31,11,13,70,3,7,21,78]])

  TrackSet_1 = [(BASE_PATH)+'/'+class_+'/%$'%(x) for x in train_music_1]

  TrackSet_2 = [(BASE_PATH)+'/'+class_+'/%$'%(x) for x in train_music_2]


  return TrackSet_1, TrackSet_2

def load(file_):
  data_, sampling_rate= librosa.load(file_,sr=3000, offset=0.0, duration=30)

  data_ = data_.reshape(1,90001)
  return data_

map_data = lambda filename: tf.compat.v1.py_func(load, [filename], [tf.float32])

TrackSet_1, TrackSet_2 = DatasetLoader('jazz')



"""sample original **music**"""

sample = TrackSet_1[1]

sample_, sampling_rate= librosa.load(sample, sr=3000, offset=0.0, duration=30)

ipd.audio (sample_,rate=3000)

import librosa.display
plt.figure(figsize=(18,15))

for i in range(4):
  plt.subplot(4,4,i+1)
  j = load(TrackSet_1[i])
  librosa.display.AdaptiveWaveplot(j[0], sr=3000)

train_dataset =(
    tf.data.Dataset.from_tensor_slices((TrackSet_1)).map(map_data, num_parallel_calls=AUTOTUNE).shuffle(3).batch(BATCH_SIZE)
)

test_dataset=(
    tf.data.Dataset.from_tensor_slices((TrackSet_2))
    .map(map_data, num_parallel_calls=AUTOTUNE)
    .shuffle(3)
    .batch(BATCH_SIZE)
)





"""Network architecture

"""

class resnet1Dblock(tf.keras.Model):
  def __init__(self, kernel_size, filters, type='encode'):
    super(Resenet1DBlock, self).__init__(name='')

    if type=='encode':
      self.conv1a = layers.Conv1D(filters, kernel_size, 2,padding="same")
      self.conv1b = layers.Conv1D(filters, kernel_size, 1,padding="same")

      self.norm1a = tfa.layers.InstanceNormalization()
      self.norm1b = tfa.layers.InstanceNormalization()

    if type=='decode':
      self.conv1a = layers.Conv1DTranspose(filters,kernel_size, 1,padding="same")
      self.conv1b = layers.Conv1DTranspose(filters,kernel_size, 1,padding="same")

      self.norm1a = tf.keras.layers.BatchNormalization()
      self.norm1b = tf.keras.layers.BatchNormalization()

    else:

        return None
  def call(self, input_tensor):
    x = tf.nn.relu(input_tensor)
    x = self.cova1a(x)
    x = self.norm1a(x)
    x = layers.LeakyReLU(0.4)(x)

    x = self.conv1b(x)
    x = self.norm1b(x)
    x = layers.LeakyReLU(0.4)(x)


    x += input_tensor
    return tf.nn.relu(x)

class CVAE(tf.keras.Model):
  """convolutional variational autoencoder."""
  def __init__(self, latent_dim):
    super(CVAE, self).__init__()
    self.latent_dim = latent_dim
    self.encoder = tf.keras.Sequential(
        [
            tf.keras.layers.InputLayer(input_shape=(1,90001)),

            layers.Conv1D(64,1,2),
            Resnet1DBlock(64,1),
            layers.Conv1D(128,1,2),
            Resnet1DBlock(128,1),
            layers.Conv1D(128,1,2),
            Resnet1DBlock(128,1),
            layers.Conv1D(256,1,2),
            Resnet1DBlock(256,1),

            layers.Flatten(),
            layers.Dense(latent_dim+latent_dim)

        ]
    )

    self.decoder = tf.keras.sequential(
        [
            tf.keras.layers.InputLayer(input_shape=(latent_dim,)),

            layers.Reshape(target_shape=(1,latent_dim)),
            Resnet1DBlock(512,1,'decode'),
            layers.Conv1DTranspose(512,1,1),
            Resnet1DBlock(256,1,'decode'),
            layers.Conv1DTranspose(256,1,1),
            Resnet1DBlock(128,1,'decode'),
            layers.Conv1DTranspose(128,1,1),
            Resnet1DBlock(64,1,'decode'),
            layers.Conv1DTranspose(64,1,1),

            layers.Conv1DTranspose(90001,1,1),

        ]
    )

  @tf.function
  def sample(self, esp=None):
      if eps is None:
        eps = tf.random.normal(shape=(200, self.latent_dim))
      return self.decode(eps, apply_sigmoid=True)
  @tf.function
  def encode(self,x):
    mean, logvar = tf.split(self.encoder(x), num_or_size_splits=2, axis=1)
    return mean, logvar

  @tf.function
  def reparameterize(self, mean, logvar):
    eps = tf.random.normal(shape=mean.shape)
    return eps* tf.exp(logvar* .5)+ mean
  @tf.function
  def decode(self, z, apply_sigmoid=False):
    logits = self.decoder(z)
    if apply_sigmoid:
      probs= tf.sigmoid(logits)
      return probs
    return logits

optimizer = tf.keras.optimizers.Adam(0.0003,beta_1=0.9, beta_2=0.999, epsilon=1e-08)

@tf.function
def log_normal_pdf(sample, mean, logvar, raxis=1):
  log2pi = tf.math.log(2.* np.pi)
  return tf.reduce_sum(
      -.5* ((sample - mean)**2. * tf.exp(-logvar) + logvar + log2pi),
      axis = raxis
  )

@tf.function
def compute_loss(model,x):
  mean, logvar = model.encode(x)
  z = model.reparameterize(mean, logvar)
  x_logit = model.decode(z)
  cross_ent = tf.nn.sigmoid_cross_entropy_with_logits(logits=x_logit, labels=x)

  logpx_z = -tf.reduce_sum(cross_ent, axis=[1,2])
  logpz = log_normal_pdf(z, 0., 0.)
  logqz_x = log_normal_pdf(z, mean, logvar)
  return -tf.reduce_mean(logpx_z + logpz - logqz_x)

"""LOSS OPTIMIZATION"""

